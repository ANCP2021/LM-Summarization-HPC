# LM-FeatureExtraction-Summarization-HPC

## Evaluating Language Models for Feature Extraction and Summarization on High Performance Computing Systems

### Alexander Nemecek, Joseph Simon, Samantha Devtski, Suvaion Das, & Vikram Singh

In this research project, we aim to explore the different aspects and performance capabilities of various transformer-based language models in feature extraction and summarization tasks. Leveraging Case Western Reserve Universityâ€™s high-performance computing (HPC) infrastructure, we will conduct an extensive analysis using an ensemble of open source, large language models (LLMs) from the HuggingFace repository, including Generative Pre-trained Transformer (GPT), Large Language Model Meta AI (LLaMA), Text-to-Text Transfer Transformer (T5), Robustly Optimized BERT-Pretraining Approach (RoBERTa), and XLNet. Our study will involve deploying these models on GPU clusters to maximize performance and efficiency in order to handle extensive data requirements. The primary objectives of the research project are (i) setting up and running each LLM on HPC resources to perform feature extraction and summarization tasks on a standardized dataset, (ii) evaluating the outputs of each model to understand qualitative differences in feature extractions and summary generations, and (iii) quantifying model performance using a set of evaluation metrics (e.g., execution time, memory usage, etc.). We aim to understand the strengths and weaknesses of each LLM in a feature extraction and summarization task context and how different transformer-based architectures impact text generation and efficiency of their executions on HPC resources.
